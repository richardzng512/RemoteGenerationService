"""Mock LLM service: configurable delays, error rate, and response templates."""

import asyncio
import json
import random
import time
import uuid
from typing import AsyncGenerator

from fastapi import HTTPException
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.models.settings import ResponseTemplate
from app.schemas.openai_compat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatMessage,
    UsageInfo,
)

FALLBACK_RESPONSES = [
    "This is a mock response from RemoteGenerationService.",
    "I am a simulated AI assistant. Your request has been received and processed.",
    "Mock mode is active. You can add custom response templates in Settings.",
    "This response was generated by the RemoteGenerationService mock LLM endpoint.",
]


class MockChatService:
    async def _get_delay(self) -> float:
        return random.uniform(settings.mock_llm_delay_min, settings.mock_llm_delay_max)

    async def _should_error(self) -> bool:
        return random.random() < settings.mock_error_rate

    async def _pick_response(self, db: AsyncSession) -> str:
        result = await db.execute(
            select(ResponseTemplate).where(
                ResponseTemplate.job_type == "llm",
                ResponseTemplate.is_active == True,  # noqa: E712
            )
        )
        templates = result.scalars().all()
        if templates:
            return random.choice(templates).template_content
        return random.choice(FALLBACK_RESPONSES)

    async def generate_response(
        self, request: ChatCompletionRequest, db: AsyncSession
    ) -> ChatCompletionResponse:
        delay = await self._get_delay()
        await asyncio.sleep(delay)

        if await self._should_error():
            raise HTTPException(
                status_code=500,
                detail={
                    "error": {
                        "message": "Mock error: simulated server failure",
                        "type": "server_error",
                        "code": "mock_error",
                    }
                },
            )

        content = await self._pick_response(db)
        prompt_tokens = sum(len(m.content.split()) for m in request.messages)
        completion_tokens = len(content.split())

        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:12]}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content=content),
                    finish_reason="stop",
                )
            ],
            usage=UsageInfo(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
            ),
        )

    async def stream_response(
        self, request: ChatCompletionRequest, db: AsyncSession
    ) -> AsyncGenerator[str, None]:
        delay = await self._get_delay()
        content = await self._pick_response(db)
        words = content.split()
        chunk_id = f"chatcmpl-{uuid.uuid4().hex[:12]}"
        created = int(time.time())
        per_word_delay = delay / max(len(words), 1)

        if await self._should_error():
            error_chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {},
                        "finish_reason": "error",
                    }
                ],
                "error": {"message": "Mock error: simulated failure", "code": "mock_error"},
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            yield "data: [DONE]\n\n"
            return

        # Role chunk
        role_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request.model,
            "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
        }
        yield f"data: {json.dumps(role_chunk)}\n\n"

        for i, word in enumerate(words):
            await asyncio.sleep(per_word_delay)
            text = word + (" " if i < len(words) - 1 else "")
            chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": request.model,
                "choices": [{"index": 0, "delta": {"content": text}, "finish_reason": None}],
            }
            yield f"data: {json.dumps(chunk)}\n\n"

        # Final chunk
        done_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request.model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
        yield f"data: {json.dumps(done_chunk)}\n\n"
        yield "data: [DONE]\n\n"
